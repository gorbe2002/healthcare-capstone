{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyPLqqptiGa6lLMJa1gKnNg3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nf3l_Su3Kd-w","executionInfo":{"status":"ok","timestamp":1747084770190,"user_tz":240,"elapsed":3994,"user":{"displayName":"Danielle Grunwald","userId":"18017908855650010663"}},"outputId":"b3435e3a-2cf6-4033-c3ad-8894881b0a60"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":1}],"source":["import torch\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"]},{"cell_type":"code","source":["!pip install optuna"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tw2L_M5yKpTs","executionInfo":{"status":"ok","timestamp":1747084776171,"user_tz":240,"elapsed":3438,"user":{"displayName":"Danielle Grunwald","userId":"18017908855650010663"}},"outputId":"7a476443-52eb-493d-de84-c746e50b6163"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting optuna\n","  Downloading optuna-4.3.0-py3-none-any.whl.metadata (17 kB)\n","Collecting alembic>=1.5.0 (from optuna)\n","  Downloading alembic-1.15.2-py3-none-any.whl.metadata (7.3 kB)\n","Collecting colorlog (from optuna)\n","  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n","Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.40)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n","Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n","Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.1)\n","Downloading optuna-4.3.0-py3-none-any.whl (386 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading alembic-1.15.2-py3-none-any.whl (231 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n","Installing collected packages: colorlog, alembic, optuna\n","Successfully installed alembic-1.15.2 colorlog-6.9.0 optuna-4.3.0\n"]}]},{"cell_type":"markdown","source":["Make sure you upload the target, feature and index bladder numpy files"],"metadata":{"id":"SdH-6HN-Lzzg"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from tqdm import tqdm\n","from sklearn.metrics import mean_squared_error, r2_score\n","from sklearn.model_selection import train_test_split\n","import plotly.graph_objects as go\n","from sklearn.preprocessing import StandardScaler, RobustScaler\n","import json\n","import optuna\n","import matplotlib.pyplot as plt\n","from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n","import torch.nn.functional as F\n","import os\n","\n","os.makedirs('/content/models', exist_ok=True)\n","\n","model_type = 'bladder'\n","\n","# Load data\n","features = np.load(f'/content/{model_type}_features.npy')\n","targets = np.load(f'/content/{model_type}_target.npy')\n","index = np.load(f'/content/{model_type}_index.npy')\n","\n","class R2Loss(nn.Module):\n","    def __init__(self):\n","        super(R2Loss, self).__init__()\n","\n","    def forward(self, pred, target):\n","        target_mean = torch.mean(target)\n","        ss_tot = torch.sum((target - target_mean) ** 2) + 1e-8\n","        ss_res = torch.sum((target - pred) ** 2)\n","        r2 = 1 - (ss_res / ss_tot)\n","        return -r2\n","class WeightedR2MSELoss(nn.Module):\n","    def __init__(self, r2_weight=0.8, mse_weight=0.2, r2_scaling=1.0, mse_scaling=1000000.0):\n","        super(WeightedR2MSELoss, self).__init__()\n","        self.r2_weight = r2_weight\n","        self.mse_weight = mse_weight\n","        self.r2_scaling = r2_scaling\n","        self.mse_scaling = mse_scaling\n","\n","    def forward(self, pred, target):\n","        mse = torch.mean((pred - target) ** 2)\n","        target_mean = torch.mean(target)\n","        ss_tot = torch.sum((target - target_mean) ** 2) + 1e-8\n","        ss_res = torch.sum((target - pred) ** 2)\n","        r2 = 1 - (ss_res / ss_tot)\n","        return -self.r2_weight * self.r2_scaling * r2 + self.mse_weight * self.mse_scaling * mse\n","\n","class TimeSeries(Dataset):\n","    def __init__(self, X: np.ndarray, y: np.ndarray, scale_y: bool = True, noise_level=0.0001):\n","        assert X.shape[0] == y.shape[0], \"Mismatched number of samples\"\n","\n","        self.n, self.t, self.f = X.shape\n","        X = np.nan_to_num(X, nan=0.0, posinf=1e6, neginf=-1e6)\n","        self.x_scaler = RobustScaler(quantile_range=(5.0, 95.0))\n","        X_flat = X.reshape(-1, self.f)\n","        self.X_scaled = self.x_scaler.fit_transform(X_flat).reshape(self.n, self.t, self.f)\n","        self.X_scaled = np.clip(self.X_scaled, -10.0, 10.0)\n","        if noise_level > 0:\n","            self.X_scaled = self.X_scaled + np.random.normal(0, noise_level, self.X_scaled.shape)\n","\n","        if np.isnan(self.X_scaled).any():\n","            print(\"Warning: NaNs detected in X_scaled after preprocessing\")\n","            self.X_scaled = np.nan_to_num(self.X_scaled, nan=0.0)\n","\n","        self.scale_y = scale_y\n","        if y.ndim == 1:\n","            y = y.reshape(-1, 1)\n","\n","        y = np.nan_to_num(y, nan=0.0, posinf=1e6, neginf=-1e6)\n","\n","        if scale_y:\n","            self.y_scaler = RobustScaler(quantile_range=(5.0, 95.0))\n","            self.y_scaled = self.y_scaler.fit_transform(y)\n","            self.y_scaled = np.clip(self.y_scaled, -10.0, 10.0)\n","        else:\n","            self.y_scaled = y\n","            self.y_scaler = None\n","\n","        if np.isnan(self.y_scaled).any():\n","            print(\"Warning: NaNs detected in y_scaled after preprocessing\")\n","            self.y_scaled = np.nan_to_num(self.y_scaled, nan=0.0)\n","\n","    def __len__(self):\n","        return self.n\n","\n","    def __getitem__(self, idx):\n","        return (\n","            torch.tensor(self.X_scaled[idx], dtype=torch.float32),\n","            torch.tensor(self.y_scaled[idx], dtype=torch.float32).view(-1)\n","        )\n","\n","    def inverse_transform_y(self, y_tensor: torch.Tensor) -> np.ndarray:\n","        if self.scale_y and self.y_scaler is not None:\n","            return self.y_scaler.inverse_transform(y_tensor.detach().cpu().numpy())\n","        else:\n","            return y_tensor.detach().cpu().numpy()\n","\n","\n","class AttentionLSTM(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2, bidirectional=False):\n","        super(AttentionLSTM, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.bidirectional = bidirectional\n","        dir_factor = 2 if bidirectional else 1\n","        self.lstm = nn.LSTM(\n","            input_size=input_size,\n","            hidden_size=hidden_size,\n","            num_layers=num_layers,\n","            batch_first=True,\n","            dropout=dropout if num_layers > 1 else 0,\n","            bidirectional=bidirectional\n","        )\n","        self.attention = nn.Sequential(\n","            nn.Linear(hidden_size * dir_factor, hidden_size),\n","            nn.Tanh(),\n","            nn.Linear(hidden_size, 1),\n","            nn.Softmax(dim=1)\n","        )\n","        self.fc1 = nn.Linear(hidden_size * dir_factor, hidden_size)\n","        self.bn1 = nn.BatchNorm1d(hidden_size)\n","        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n","        self.bn2 = nn.BatchNorm1d(hidden_size // 2)\n","        self.fc3 = nn.Linear(hidden_size // 2, output_size)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        if torch.isnan(x).any():\n","            x = torch.nan_to_num(x, nan=0.0)\n","        lstm_out, _ = self.lstm(x)\n","\n","        if torch.isnan(lstm_out).any():\n","            lstm_out = torch.nan_to_num(lstm_out, nan=0.0)\n","\n","        attention_weights = self.attention(lstm_out)\n","        context = torch.sum(attention_weights * lstm_out, dim=1)\n","\n","        out = self.fc1(context)\n","        if out.shape[0] > 1:\n","            out = self.bn1(out)\n","        out = F.gelu(out)\n","        out = self.dropout(out)\n","        out = self.fc2(out)\n","        if out.shape[0] > 1:\n","            out = self.bn2(out)\n","        out = F.gelu(out)\n","        out = self.dropout(out)\n","        out = self.fc3(out)\n","        if torch.isnan(out).any():\n","            out = torch.nan_to_num(out, nan=0.0)\n","\n","        return out\n","\n","\n","class EnsembleModel(nn.Module):\n","    def __init__(self, models, weights=None):\n","        super(EnsembleModel, self).__init__()\n","        self.models = nn.ModuleList(models)\n","        if weights is None:\n","            self.weights = torch.ones(len(models)) / len(models)\n","        else:\n","            total = sum(weights)\n","            self.weights = torch.tensor([w/total for w in weights])\n","\n","    def forward(self, x):\n","        outputs = []\n","        for i, model in enumerate(self.models):\n","            outputs.append(model(x))\n","        ensemble_output = torch.zeros_like(outputs[0])\n","        for i, output in enumerate(outputs):\n","            ensemble_output += self.weights[i] * output\n","        return ensemble_output\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {device}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LsdY1AMJKq1P","executionInfo":{"status":"ok","timestamp":1747084806869,"user_tz":240,"elapsed":1287,"user":{"displayName":"Danielle Grunwald","userId":"18017908855650010663"}},"outputId":"d26862ed-d603-4c45-ef7c-b9b6339c24bb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}]},{"cell_type":"code","source":["\n","def train(model, criterion, optimizer, train_dataloader, scheduler=None):\n","    model.train()\n","    epoch_loss = 0.0\n","    for batch_X, batch_y in train_dataloader:\n","        if torch.isnan(batch_X).any() or torch.isnan(batch_y).any():\n","            batch_X = torch.nan_to_num(batch_X, nan=0.0)\n","            batch_y = torch.nan_to_num(batch_y, nan=0.0)\n","        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(batch_X)\n","        loss = criterion(outputs, batch_y)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","        optimizer.step()\n","        epoch_loss += loss.item()\n","\n","    if scheduler is not None:\n","        if isinstance(scheduler, ReduceLROnPlateau):\n","            scheduler.step(epoch_loss / len(train_dataloader))\n","        else:\n","            scheduler.step()\n","    return epoch_loss / len(train_dataloader)\n","\n","\n","def validate(model, val_dataloader, dataset):\n","    model.eval()\n","    all_preds = []\n","    all_targets = []\n","\n","    with torch.no_grad():\n","        for batch_X, batch_y in val_dataloader:\n","            if torch.isnan(batch_X).any() or torch.isnan(batch_y).any():\n","                batch_X = torch.nan_to_num(batch_X, nan=0.0)\n","                batch_y = torch.nan_to_num(batch_y, nan=0.0)\n","            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n","            outputs = model(batch_X)\n","            if torch.isnan(outputs).any():\n","                outputs = torch.nan_to_num(outputs, nan=0.0)\n","            all_targets.append(batch_y.cpu())\n","            all_preds.append(outputs.cpu())\n","    all_preds = torch.cat(all_preds, dim=0)\n","    all_targets = torch.cat(all_targets, dim=0)\n","    all_preds = torch.nan_to_num(all_preds, nan=0.0)\n","    all_targets = torch.nan_to_num(all_targets, nan=0.0)\n","    np_preds = all_preds.numpy()\n","    np_targets = all_targets.numpy()\n","    mse = mean_squared_error(np_targets, np_preds)\n","    r2 = r2_score(np_targets, np_preds)\n","    try:\n","        corr = np.corrcoef(np_targets.flatten(), np_preds.flatten())[0, 1]\n","    except:\n","        corr = 0.0\n","    y_pred = dataset.inverse_transform_y(all_preds)\n","    y_true = dataset.inverse_transform_y(all_targets)\n","    true_mse = mean_squared_error(y_true, y_pred)\n","    true_r2 = r2_score(y_true, y_pred)\n","\n","    return mse, r2, corr, true_mse, true_r2\n","\n","\n","def objective(trial):\n","    hidden_size = trial.suggest_int(\"hidden_size\", 64, 256, step=32)\n","    num_layers = trial.suggest_int(\"num_layers\", 1, 4)\n","    dropout = trial.suggest_float(\"dropout\", 0.0, 0.5)\n","    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n","    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32, 64])\n","    bidirectional = trial.suggest_categorical(\"bidirectional\", [True, False])\n","    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n","    r2_weight = trial.suggest_float(\"r2_weight\", 0.5, 0.95)\n","    mse_weight = 1.0 - r2_weight\n","    loss_type = trial.suggest_categorical(\"loss_type\", [\"r2\", \"weighted\"])\n","\n","    try:\n","        X_train, X_test, y_train, y_test = train_test_split(\n","            features,\n","            targets,\n","            test_size=0.2,\n","            shuffle=True,\n","            stratify=index[:, 1],\n","            random_state=42\n","        )\n","        train_data = TimeSeries(X_train, y_train, noise_level=0.0001)\n","        test_data = TimeSeries(X_test, y_test, noise_level=0)\n","        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=0)\n","        test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=0)\n","        model = AttentionLSTM(\n","            input_size=features.shape[2],\n","            hidden_size=hidden_size,\n","            num_layers=num_layers,\n","            dropout=dropout,\n","            output_size=1,\n","            bidirectional=bidirectional\n","        ).to(device)\n","\n","        if loss_type == \"r2\":\n","            criterion = R2Loss()\n","        else:\n","            criterion = WeightedR2MSELoss(r2_weight=r2_weight, mse_weight=mse_weight)\n","        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n","        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=False)\n","        epochs = 50\n","        best_r2 = -float('inf')\n","        patience = 7\n","        patience_counter = 0\n","\n","        for epoch in range(epochs):\n","            train_loss = train(model, criterion, optimizer, train_loader, scheduler)\n","            _, r2, _, _, _ = validate(model, test_loader, test_data)\n","            if r2 > best_r2:\n","                best_r2 = r2\n","                patience_counter = 0\n","            else:\n","                patience_counter += 1\n","                if patience_counter >= patience:\n","                    break\n","        _, final_r2, _, true_mse, true_r2 = validate(model, test_loader, test_data)\n","        trial.report(true_r2, epoch)\n","\n","        if trial.should_prune():\n","            raise optuna.exceptions.TrialPruned()\n","\n","        baseline_mse = 0.00000011\n","        baseline_r2 = 0.75\n","\n","\n","        if true_r2 < baseline_r2:\n","            return true_r2\n","\n","\n","        mse_factor = true_mse / baseline_mse\n","        if mse_factor > 10:\n","            penalty = 0.05 * (mse_factor / 10)\n","            return true_r2 - min(penalty, 0.1)\n","\n","        return true_r2\n","\n","    except Exception as e:\n","        print(f\"Trial failed with error: {e}\")\n","        return 0.0\n","\n","study = optuna.create_study(direction=\"maximize\",\n","                           pruner=optuna.pruners.MedianPruner(n_startup_trials=5))\n","study.optimize(objective, n_trials=50)\n","\n","print(\"Best trial:\")\n","print(f\"R² Score: {study.best_trial.value}\")\n","print(\"Best hyperparameters:\")\n","for key, value in study.best_trial.params.items():\n","    print(f\"  {key}: {value}\")\n","with open(f'/content/models/{model_type}_r2_optimized_params.json', 'w') as file:\n","    json.dump(study.best_trial.params, file)\n","params = study.best_trial.params\n","hidden_size = params['hidden_size']\n","num_layers = params['num_layers']\n","dropout = params['dropout']\n","learning_rate = params['learning_rate']\n","batch_size = params['batch_size']\n","bidirectional = params['bidirectional']\n","weight_decay = params['weight_decay']\n","loss_type = params['loss_type']\n","if 'r2_weight' in params:\n","    r2_weight = params['r2_weight']\n","    mse_weight = 1.0 - r2_weight\n","else:\n","    r2_weight = 0.8\n","    mse_weight = 0.2\n","X_train, X_test, y_train, y_test, _, index_test = train_test_split(\n","    features,\n","    targets,\n","    index,\n","    test_size=0.2,\n","    shuffle=True,\n","    stratify=index[:, 1],\n","    random_state=42\n",")\n","train_data = TimeSeries(X_train, y_train, noise_level=0.0001)\n","test_data = TimeSeries(X_test, y_test, noise_level=0)\n","train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2)\n","test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=2)\n","models = []\n","for i in range(3):\n","    h_size = int(hidden_size * (0.9 + 0.2 * np.random.random()))\n","    d_out = dropout * (0.8 + 0.4 * np.random.random())\n","\n","    model = AttentionLSTM(\n","        input_size=features.shape[2],\n","        hidden_size=h_size,\n","        num_layers=num_layers,\n","        dropout=d_out,\n","        output_size=1,\n","        bidirectional=bidirectional\n","    ).to(device)\n","\n","    models.append(model)\n","ensemble_model = EnsembleModel(models, weights=[0.5, 0.3, 0.2]).to(device)\n","\n","if loss_type == \"r2\":\n","    criterion = R2Loss()\n","else:\n","    criterion = WeightedR2MSELoss(r2_weight=r2_weight, mse_weight=mse_weight)\n","optimizer = optim.AdamW(ensemble_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n","scheduler1 = ReduceLROnPlateau(optimizer, mode='min', factor=0.7, patience=10, verbose=True, min_lr=1e-6)\n","scheduler2 = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=learning_rate/20)\n","num_epochs = 150\n","best_r2 = -float('inf')\n","best_model_state = None\n","patience = 25\n","patience_counter = 0\n","\n","train_losses = []\n","val_mses = []\n","val_r2s = []\n","val_corrs = []\n","\n","for epoch in tqdm(range(num_epochs)):\n","    train_loss = train(ensemble_model, criterion, optimizer, train_loader)\n","    scheduler2.step()\n","    mse, r2, corr, true_mse, true_r2 = validate(ensemble_model, test_loader, test_data)\n","    scheduler1.step(train_loss)\n","    train_losses.append(train_loss)\n","    val_mses.append(true_mse)\n","    val_r2s.append(true_r2)\n","    val_corrs.append(corr)\n","    if true_r2 > best_r2:\n","        best_r2 = true_r2\n","        best_model_state = {\n","            'ensemble': ensemble_model.state_dict(),\n","            'models': [model.state_dict() for model in models]\n","        }\n","        patience_counter = 0\n","    else:\n","        patience_counter += 1\n","        if patience_counter >= patience:\n","            print(f\"Early stopping at epoch {epoch+1}\")\n","            break\n","    if (epoch + 1) % 10 == 0:\n","        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.7f}, \"\n","              f\"Val MSE: {true_mse:.8f}, Val R²: {true_r2:.7f}, Val Corr: {corr:.7f}\")\n","if best_model_state:\n","    ensemble_model.load_state_dict(best_model_state['ensemble'])\n","    for i, model_state in enumerate(best_model_state['models']):\n","        models[i].load_state_dict(model_state)\n","\n","plt.figure(figsize=(15, 10))\n","\n","plt.subplot(2, 2, 1)\n","plt.plot(train_losses)\n","plt.title('Training Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","\n","plt.subplot(2, 2, 2)\n","plt.plot(val_mses)\n","plt.title('Validation MSE')\n","plt.xlabel('Epoch')\n","plt.ylabel('MSE')\n","\n","plt.subplot(2, 2, 3)\n","plt.plot(val_r2s)\n","plt.title('Validation R²')\n","plt.xlabel('Epoch')\n","plt.ylabel('R²')\n","\n","plt.subplot(2, 2, 4)\n","plt.plot(val_corrs)\n","plt.title('Validation Correlation')\n","plt.xlabel('Epoch')\n","plt.ylabel('Correlation')\n","\n","plt.tight_layout()\n","plt.savefig(f'/content/models/{model_type}_r2_optimized_training_curves.png')\n","plt.close()\n","\n","ensemble_model.eval()\n","predictions = []\n","targets = []\n","\n","with torch.no_grad():\n","    for X_batch, y_batch in test_loader:\n","        X_batch = X_batch.to(device)\n","        outputs = ensemble_model(X_batch).cpu()\n","        predictions.append(outputs)\n","        targets.append(y_batch)\n","\n","y_pred_tensor = torch.cat(predictions)\n","y_true_tensor = torch.cat(targets)\n","\n","y_pred = test_data.inverse_transform_y(y_pred_tensor)\n","y_true = test_data.inverse_transform_y(y_true_tensor)\n","\n","df = pd.DataFrame(index_test, columns=[\"state\", \"year\"])\n","df['year'] = df['year'].astype(int)\n","df[\"y_true\"] = y_true.flatten()\n","df[\"y_pred\"] = y_pred.flatten()\n","\n","yg = df.sort_values('year').groupby('year')[['y_true', 'y_pred']].mean().reset_index()\n","\n","final_mse = mean_squared_error(y_true, y_pred)\n","final_r2 = r2_score(y_true, y_pred)\n","final_corr = np.corrcoef(y_true.flatten(), y_pred.flatten())[0, 1]\n","\n","print(f\"Final Test MSE: {final_mse:.8f}\")\n","print(f\"Final Test R²: {final_r2:.7f}\")\n","print(f\"Final Test Correlation: {final_corr:.4f}\")\n","\n","fig = go.Figure()\n","\n","fig.add_scatter(x=yg[\"year\"], y=yg[\"y_pred\"], mode=\"markers+lines\", name=\"Predictions\",\n","                marker=dict(symbol=\"x\", size=10))\n","fig.add_scatter(x=yg[\"year\"], y=yg[\"y_true\"], mode=\"markers+lines\", name=\"Ground Truth\",\n","                marker=dict(size=8))\n","\n","yearly_r2 = r2_score(yg[\"y_true\"], yg[\"y_pred\"])\n","yearly_corr = np.corrcoef(yg[\"y_true\"], yg[\"y_pred\"])[0, 1]\n","\n","fig.update_layout(\n","    title=f\"{model_type.capitalize()} Cancer Model (R² Optimized) - Yearly Average (R² = {yearly_r2:.3f}, Corr = {yearly_corr:.3f})\",\n","    xaxis_title=\"Year\",\n","    yaxis_title=\"Value\",\n","    template=\"plotly_white\",\n","    legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1)\n",")\n","\n","fig.write_html(f'/content/models/{model_type}_r2_optimized_yearly_predictions.html')\n","\n","fig2 = go.Figure()\n","\n","fig2.add_scatter(x=y_true.flatten(), y=y_pred.flatten(), mode=\"markers\",\n","                marker=dict(size=8, opacity=0.6))\n","\n","max_val = max(np.max(y_true), np.max(y_pred))\n","min_val = min(np.min(y_true), np.min(y_pred))\n","fig2.add_scatter(x=[min_val, max_val], y=[min_val, max_val], mode=\"lines\",\n","                name=\"Perfect Prediction\", line=dict(dash=\"dash\", color=\"gray\"))\n","\n","fig2.update_layout(\n","    title=f\"{model_type.capitalize()} Cancer Model (R² Optimized) - Predictions vs Ground Truth (R² = {final_r2:.7f}, MSE = {final_mse:.8f})\",\n","    xaxis_title=\"Ground Truth\",\n","    yaxis_title=\"Prediction\",\n","    template=\"plotly_white\"\n",")\n","\n","fig2.write_html(f'/content/models/{model_type}_r2_optimized_scatter_predictions.html')\n","\n","torch.save({\n","    'ensemble': ensemble_model.state_dict(),\n","    'models': [model.state_dict() for model in models],\n","    'params': params\n","}, f\"/content/models/{model_type}_r2_optimized_model.pth\")\n","\n","# ----------------\n","# Two-Stage Training Approach\n","# ----------------\n","\n","print(\"\\nStarting two-stage training approach...\")\n","base_model = AttentionLSTM(\n","    input_size=features.shape[2],\n","    hidden_size=hidden_size,\n","    num_layers=num_layers,\n","    dropout=dropout,\n","    output_size=1,\n","    bidirectional=bidirectional\n",").to(device)\n","stage1_criterion = nn.MSELoss()\n","stage1_optimizer = optim.Adam(base_model.parameters(), lr=learning_rate, weight_decay=weight_decay/10)\n","stage1_scheduler = ReduceLROnPlateau(stage1_optimizer, mode='min', factor=0.5, patience=5)\n","print(\"Stage 1: MSE Optimization\")\n","stage1_epochs = 50\n","best_mse = float('inf')\n","best_stage1_state = None\n","patience_counter = 0\n","\n","for epoch in range(stage1_epochs):\n","    train_loss = train(base_model, stage1_criterion, stage1_optimizer, train_loader, stage1_scheduler)\n","    mse, r2, _, _, _ = validate(base_model, test_loader, test_data)\n","    if mse < best_mse:\n","        best_mse = mse\n","        best_stage1_state = base_model.state_dict().copy()\n","        patience_counter = 0\n","    else:\n","        patience_counter += 1\n","        if patience_counter >= 10:\n","            print(f\"Early stopping stage 1 at epoch {epoch+1}\")\n","            break\n","\n","    if (epoch + 1) % 10 == 0:\n","        print(f\"Stage 1 - Epoch {epoch+1}/{stage1_epochs}, Train Loss: {train_loss:.7f}, Val MSE: {mse:.8f}, Val R²: {r2:.7f}\")\n","base_model.load_state_dict(best_stage1_state)\n","print(\"\\nStage 2: R² Optimization\")\n","for name, param in base_model.named_parameters():\n","    if 'lstm' in name or 'fc1' in name:\n","        param.requires_grad = False\n","\n","stage2_criterion = R2Loss()\n","stage2_optimizer = optim.AdamW(\n","    filter(lambda p: p.requires_grad, base_model.parameters()),\n","    lr=learning_rate / 5,\n","    weight_decay=weight_decay / 2\n",")\n","stage2_scheduler = CosineAnnealingWarmRestarts(stage2_optimizer, T_0=5, T_mult=2, eta_min=1e-6)\n","\n","stage2_epochs = 50\n","best_r2 = -float('inf')\n","best_stage2_state = None\n","patience_counter = 0\n","\n","for epoch in range(stage2_epochs):\n","    train_loss = train(base_model, stage2_criterion, stage2_optimizer, train_loader, stage2_scheduler)\n","    _, r2, _, true_mse, true_r2 = validate(base_model, test_loader, test_data)\n","\n","    if true_r2 > best_r2:\n","        best_r2 = true_r2\n","        best_stage2_state = base_model.state_dict().copy()\n","        patience_counter = 0\n","    else:\n","        patience_counter += 1\n","        if patience_counter >= 15:\n","            print(f\"Early stopping stage 2 at epoch {epoch+1}\")\n","            break\n","\n","    if (epoch + 1) % 5 == 0:\n","        print(f\"Stage 2 - Epoch {epoch+1}/{stage2_epochs}, Train Loss: {train_loss:.7f}, Val R²: {true_r2:.7f}, Val MSE: {true_mse:.8f}\")\n","\n","base_model.load_state_dict(best_stage2_state)\n","\n","_, _, _, two_stage_mse, two_stage_r2 = validate(base_model, test_loader, test_data)\n","\n","print(\"\\nTwo-Stage Model Results:\")\n","print(f\"MSE: {two_stage_mse:.8f}\")\n","print(f\"R²: {two_stage_r2:.7f}\")\n","\n","torch.save(base_model.state_dict(), f\"/content/models/{model_type}_two_stage_model.pth\")\n","\n","# ----------------\n","# Trend-focused Model\n","# ----------------\n","\n","print(\"\\nTraining trend-focused model...\")\n","class TrendDataset(Dataset):\n","    def __init__(self, X: np.ndarray, y: np.ndarray, scale_y: bool = True):\n","        assert X.shape[0] == y.shape[0], \"Mismatched number of samples\"\n","        self.original_X = X\n","        self.original_y = y.reshape(-1, 1) if y.ndim == 1 else y\n","        self.x_scaler = RobustScaler(quantile_range=(5.0, 95.0))\n","        X_flat = np.nan_to_num(X.reshape(-1, X.shape[2]), nan=0.0)\n","        X_scaled = self.x_scaler.fit_transform(X_flat).reshape(X.shape)\n","        X_scaled = np.clip(X_scaled, -10.0, 10.0)\n","        self.n, self.t, self.f = X.shape\n","        self.features = []\n","        self.targets = []\n","        self.indices = []\n","        for i in range(self.n):\n","            x_sample = X_scaled[i]\n","            y_sample = y[i]\n","            if np.isnan(x_sample).any() or np.isnan(y_sample):\n","                continue\n","            if self.t > 1:\n","                x_pct_change = np.zeros((self.t-1, self.f))\n","                for j in range(self.t-1):\n","                    denom = np.abs(x_sample[j]) + 1e-8\n","                    x_pct_change[j] = (x_sample[j+1] - x_sample[j]) / denom\n","                x_pct_change = np.clip(x_pct_change, -10.0, 10.0)\n","                x_combined = np.column_stack([\n","                    x_pct_change.reshape(-1),\n","                    x_sample[:-1].reshape(-1)\n","                ])\n","\n","                self.features.append(x_combined)\n","                self.targets.append(y_sample)\n","                self.indices.append(i)\n","\n","        if len(self.features) > 0:\n","            self.features = np.array(self.features)\n","            self.targets = np.array(self.targets).reshape(-1, 1)\n","            self.indices = np.array(self.indices)\n","            self.scale_y = scale_y\n","            if scale_y:\n","                self.y_scaler = RobustScaler(quantile_range=(5.0, 95.0))\n","                self.targets_scaled = self.y_scaler.fit_transform(self.targets)\n","                self.targets_scaled = np.clip(self.targets_scaled, -10.0, 10.0)\n","            else:\n","                self.targets_scaled = self.targets\n","                self.y_scaler = None\n","        else:\n","            print(\"Warning: No valid samples found for trend dataset\")\n","            self.features = np.zeros((1, 1, self.f * 2))\n","            self.targets = np.zeros((1, 1))\n","            self.targets_scaled = np.zeros((1, 1))\n","            self.indices = np.zeros(1)\n","            self.scale_y = scale_y\n","            self.y_scaler = None\n","\n","    def __len__(self):\n","        return len(self.features)\n","\n","    def __getitem__(self, idx):\n","        return (\n","            torch.tensor(self.features[idx], dtype=torch.float32),\n","            torch.tensor(self.targets_scaled[idx], dtype=torch.float32).view(-1)\n","        )\n","\n","    def inverse_transform_y(self, y_tensor: torch.Tensor) -> np.ndarray:\n","        if self.scale_y and self.y_scaler is not None:\n","            return self.y_scaler.inverse_transform(y_tensor.detach().cpu().numpy())\n","        else:\n","            return y_tensor.detach().cpu().numpy()\n","class TrendModel(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size, dropout=0.2):\n","        super(TrendModel, self).__init__()\n","        self.fc1 = nn.Linear(input_size, hidden_size)\n","        self.bn1 = nn.BatchNorm1d(hidden_size)\n","\n","        self.fc2 = nn.Linear(hidden_size, hidden_size)\n","        self.bn2 = nn.BatchNorm1d(hidden_size)\n","\n","        self.fc3 = nn.Linear(hidden_size, hidden_size // 2)\n","        self.bn3 = nn.BatchNorm1d(hidden_size // 2)\n","\n","        self.fc4 = nn.Linear(hidden_size // 2, output_size)\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        batch_size = x.size(0)\n","        if torch.isnan(x).any():\n","            x = torch.nan_to_num(x, nan=0.0)\n","        if x.dim() > 2:\n","            x = x.reshape(batch_size, -1)\n","        x = self.fc1(x)\n","        if batch_size > 1:\n","            x = self.bn1(x)\n","        x = F.gelu(x)\n","        x = self.dropout(x)\n","        identity = x\n","        x = self.fc2(x)\n","        if batch_size > 1:\n","            x = self.bn2(x)\n","        x = F.gelu(x)\n","        x = self.dropout(x)\n","        x = x + identity\n","        x = self.fc3(x)\n","        if batch_size > 1:\n","            x = self.bn3(x)\n","        x = F.gelu(x)\n","        x = self.dropout(x)\n","        x = self.fc4(x)\n","        if torch.isnan(x).any():\n","            x = torch.nan_to_num(x, nan=0.0)\n","\n","        return x\n","trend_train_data = TrendDataset(X_train, y_train)\n","trend_test_data = TrendDataset(X_test, y_test)\n","input_dim = trend_train_data.features.shape[1] * trend_train_data.features.shape[2]\n","trend_train_loader = DataLoader(trend_train_data, batch_size=32, shuffle=True, num_workers=2)\n","trend_test_loader = DataLoader(trend_test_data, batch_size=32, shuffle=False, num_workers=2)\n","trend_model = TrendModel(\n","    input_size=input_dim,\n","    hidden_size=128,\n","    output_size=1,\n","    dropout=0.2\n",").to(device)\n","trend_criterion = R2Loss()\n","trend_optimizer = optim.AdamW(trend_model.parameters(), lr=0.001, weight_decay=1e-5)\n","trend_scheduler = ReduceLROnPlateau(trend_optimizer, mode='min', factor=0.5, patience=10, verbose=True)\n","trend_epochs = 100\n","best_trend_r2 = -float('inf')\n","best_trend_state = None\n","patience = 20\n","patience_counter = 0\n","print(\"\\nTraining trend-focused model...\")\n","for epoch in range(trend_epochs):\n","    train_loss = train(trend_model, trend_criterion, trend_optimizer, trend_train_loader, trend_scheduler)\n","    _, r2, _, true_mse, true_r2 = validate(trend_model, trend_test_loader, trend_test_data)\n","\n","    if true_r2 > best_trend_r2:\n","        best_trend_r2 = true_r2\n","        best_trend_state = trend_model.state_dict().copy()\n","        patience_counter = 0\n","    else:\n","        patience_counter += 1\n","        if patience_counter >= patience:\n","            print(f\"Early stopping trend model at epoch {epoch+1}\")\n","            break\n","    if (epoch + 1) % 10 == 0:\n","        print(f\"Trend Model - Epoch {epoch+1}/{trend_epochs}, Train Loss: {train_loss:.7f}, \"\n","              f\"Val R²: {true_r2:.7f}, Val MSE: {true_mse:.8f}\")\n","\n","trend_model.load_state_dict(best_trend_state)\n","_, _, _, trend_mse, trend_r2 = validate(trend_model, trend_test_loader, trend_test_data)\n","\n","print(\"\\nTrend Model Results:\")\n","print(f\"MSE: {trend_mse:.8f}\")\n","print(f\"R²: {trend_r2:.7f}\")\n","\n","torch.save(trend_model.state_dict(), f\"/content/models/{model_type}_trend_model.pth\")\n","\n","# ----------------\n","# Ultra Ensemble - Final Solution\n","# ----------------\n","\n","print(\"\\nCreating ultra ensemble solution...\")\n","def ultra_ensemble_predict(X_batch, weights=None):\n","    \"\"\"\n","    Combines predictions from all three model approaches:\n","    1. Ensemble LSTM model\n","    2. Two-stage model\n","    3. Trend model\n","\n","    Returns weighted average of predictions\n","    \"\"\"\n","    if weights is None:\n","        weights = [0.5, 0.3, 0.2]\n","    X_batch = X_batch.to(device)\n","    ensemble_model.eval()\n","    with torch.no_grad():\n","        ensemble_pred = ensemble_model(X_batch).cpu()\n","    base_model.eval()\n","    with torch.no_grad():\n","        two_stage_pred = base_model(X_batch).cpu()\n","    X_np = X_batch.cpu().numpy()\n","    n, t, f = X_np.shape\n","    trend_features = []\n","\n","    for i in range(n):\n","        x_sample = X_np[i]\n","        if t > 1:\n","            x_pct_change = np.zeros((t-1, f))\n","            for j in range(t-1):\n","                denom = np.abs(x_sample[j]) + 1e-8\n","                x_pct_change[j] = (x_sample[j+1] - x_sample[j]) / denom\n","            x_pct_change = np.clip(x_pct_change, -10.0, 10.0)\n","            x_combined = np.column_stack([\n","                x_pct_change.reshape(-1),\n","                x_sample[:-1].reshape(-1)\n","            ])\n","\n","            trend_features.append(x_combined)\n","    if trend_features:\n","        trend_features = torch.tensor(np.array(trend_features), dtype=torch.float32).to(device)\n","        trend_model.eval()\n","        with torch.no_grad():\n","            trend_pred = trend_model(trend_features).cpu()\n","    else:\n","        trend_pred = (ensemble_pred + two_stage_pred) / 2\n","    final_pred = (\n","        weights[0] * ensemble_pred +\n","        weights[1] * two_stage_pred +\n","        weights[2] * trend_pred\n","    )\n","\n","    return final_pred\n","\n","weight_options = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n","best_weights = None\n","best_ultra_r2 = -float('inf')\n","\n","print(\"Optimizing ultra ensemble weights...\")\n","for w1 in weight_options:\n","    for w2 in weight_options:\n","        for w3 in weight_options:\n","            if abs(w1 + w2 + w3 - 1.0) > 1e-6:\n","                continue\n","\n","            weights = [w1, w2, w3]\n","\n","            all_preds = []\n","            all_targets = []\n","\n","            for X_batch, y_batch in test_loader:\n","                pred = ultra_ensemble_predict(X_batch, weights)\n","                all_preds.append(pred)\n","                all_targets.append(y_batch)\n","\n","            y_pred_tensor = torch.cat(all_preds)\n","            y_true_tensor = torch.cat(all_targets)\n","\n","            y_pred = test_data.inverse_transform_y(y_pred_tensor)\n","            y_true = test_data.inverse_transform_y(y_true_tensor)\n","\n","            ultra_r2 = r2_score(y_true, y_pred)\n","\n","            if ultra_r2 > best_ultra_r2:\n","                best_ultra_r2 = ultra_r2\n","                best_weights = weights\n","\n","                ultra_mse = mean_squared_error(y_true, y_pred)\n","\n","                print(f\"New best weights: {weights} - R²: {ultra_r2:.7f}, MSE: {ultra_mse:.8f}\")\n","\n","print(f\"\\nFinal ultra ensemble with weights: {best_weights}\")\n","\n","all_preds = []\n","all_targets = []\n","\n","for X_batch, y_batch in test_loader:\n","    pred = ultra_ensemble_predict(X_batch, best_weights)\n","    all_preds.append(pred)\n","    all_targets.append(y_batch)\n","\n","y_pred_tensor = torch.cat(all_preds)\n","y_true_tensor = torch.cat(all_targets)\n","\n","y_pred = test_data.inverse_transform_y(y_pred_tensor)\n","y_true = test_data.inverse_transform_y(y_true_tensor)\n","\n","ultra_mse = mean_squared_error(y_true, y_pred)\n","ultra_r2 = r2_score(y_true, y_pred)\n","ultra_corr = np.corrcoef(y_true.flatten(), y_pred.flatten())[0, 1]\n","\n","print(\"\\nUltra Ensemble Final Results:\")\n","print(f\"MSE: {ultra_mse:.8f}\")\n","print(f\"R²: {ultra_r2:.7f}\")\n","print(f\"Correlation: {ultra_corr:.4f}\")\n","\n","df_ultra = pd.DataFrame(index_test, columns=[\"state\", \"year\"])\n","df_ultra['year'] = df_ultra['year'].astype(int)\n","df_ultra[\"y_true\"] = y_true.flatten()\n","df_ultra[\"y_pred\"] = y_pred.flatten()\n","yg_ultra = df_ultra.sort_values('year').groupby('year')[['y_true', 'y_pred']].mean().reset_index()\n","yearly_ultra_r2 = r2_score(yg_ultra[\"y_true\"], yg_ultra[\"y_pred\"])\n","yearly_ultra_corr = np.corrcoef(yg_ultra[\"y_true\"], yg_ultra[\"y_pred\"])[0, 1]\n","fig_ultra = go.Figure()\n","\n","fig_ultra.add_scatter(x=yg_ultra[\"year\"], y=yg_ultra[\"y_pred\"], mode=\"markers+lines\", name=\"Predictions\",\n","                marker=dict(symbol=\"x\", size=10))\n","fig_ultra.add_scatter(x=yg_ultra[\"year\"], y=yg_ultra[\"y_true\"], mode=\"markers+lines\", name=\"Ground Truth\",\n","                marker=dict(size=8))\n","\n","fig_ultra.update_layout(\n","    title=f\"{model_type.capitalize()} Cancer Model (Ultra Ensemble) - Yearly Average (R² = {yearly_ultra_r2:.3f}, Corr = {yearly_ultra_corr:.3f})\",\n","    xaxis_title=\"Year\",\n","    yaxis_title=\"Value\",\n","    template=\"plotly_white\",\n","    legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1)\n",")\n","\n","fig_ultra.write_html(f'/content/models/{model_type}_ultra_ensemble_yearly_predictions.html')\n","fig_ultra2 = go.Figure()\n","\n","fig_ultra2.add_scatter(x=y_true.flatten(), y=y_pred.flatten(), mode=\"markers\",\n","                marker=dict(size=8, opacity=0.6))\n","\n","max_val = max(np.max(y_true), np.max(y_pred))\n","min_val = min(np.min(y_true), np.min(y_pred))\n","fig_ultra2.add_scatter(x=[min_val, max_val], y=[min_val, max_val], mode=\"lines\",\n","                name=\"Perfect Prediction\", line=dict(dash=\"dash\", color=\"gray\"))\n","\n","fig_ultra2.update_layout(\n","    title=f\"{model_type.capitalize()} Cancer Model (Ultra Ensemble) - Predictions vs Ground Truth (R² = {ultra_r2:.7f}, MSE: {ultra_mse:.8f})\",\n","    xaxis_title=\"Ground Truth\",\n","    yaxis_title=\"Prediction\",\n","    template=\"plotly_white\"\n",")\n","\n","fig_ultra2.write_html(f'/content/models/{model_type}_ultra_ensemble_scatter_predictions.html')\n","torch.save({\n","    'ensemble_model': ensemble_model.state_dict(),\n","    'two_stage_model': base_model.state_dict(),\n","    'trend_model': trend_model.state_dict(),\n","    'weights': best_weights,\n","    'params': params\n","}, f\"/content/models/{model_type}_ultra_ensemble_model.pth\")\n","\n","\n","print(\"\\n=== FINAL MODEL COMPARISON ===\")\n","print(f\"Ensemble model:     R² = {final_r2:.7f}, MSE = {final_mse:.8f}\")\n","print(f\"Two-stage model:    R² = {two_stage_r2:.7f}, MSE = {two_stage_mse:.8f}\")\n","print(f\"Trend model:        R² = {trend_r2:.7f}, MSE = {trend_mse:.8f}\")\n","print(f\"Ultra ensemble:     R² = {ultra_r2:.7f}, MSE = {ultra_mse:.8f}\")\n","print(\"============================\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1hz5zR1IMFx2","executionInfo":{"status":"ok","timestamp":1747085551260,"user_tz":240,"elapsed":739797,"user":{"displayName":"Danielle Grunwald","userId":"18017908855650010663"}},"outputId":"b39c43da-4602-4199-81fe-21ffcb684c6a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[I 2025-05-12 21:20:11,761] A new study created in memory with name: no-name-82ca9734-0a5c-4417-bbb5-4aba8b2f616d\n","/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:20:39,188] Trial 0 finished with value: 0.4226493239402771 and parameters: {'hidden_size': 128, 'num_layers': 3, 'dropout': 0.46793601865559203, 'learning_rate': 0.0008785847239827912, 'batch_size': 8, 'bidirectional': True, 'weight_decay': 3.5253726320804834e-06, 'r2_weight': 0.6287221417713786, 'loss_type': 'r2'}. Best is trial 0 with value: 0.4226493239402771.\n","/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:20:46,095] Trial 1 finished with value: 0.357448935508728 and parameters: {'hidden_size': 128, 'num_layers': 4, 'dropout': 0.3802951504256747, 'learning_rate': 0.0022414335335319328, 'batch_size': 32, 'bidirectional': False, 'weight_decay': 8.491953218087513e-06, 'r2_weight': 0.6613172442562795, 'loss_type': 'weighted'}. Best is trial 0 with value: 0.4226493239402771.\n","/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:20:57,214] Trial 2 finished with value: 0.2964203357696533 and parameters: {'hidden_size': 96, 'num_layers': 4, 'dropout': 0.29288435768194016, 'learning_rate': 0.00020070538876369528, 'batch_size': 16, 'bidirectional': False, 'weight_decay': 0.0003332844578978636, 'r2_weight': 0.8101336521139897, 'loss_type': 'r2'}. Best is trial 0 with value: 0.4226493239402771.\n","/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:21:39,099] Trial 3 finished with value: 0.49815940856933594 and parameters: {'hidden_size': 256, 'num_layers': 4, 'dropout': 0.36147896609391594, 'learning_rate': 0.0016004488585221158, 'batch_size': 8, 'bidirectional': False, 'weight_decay': 4.314400780546697e-06, 'r2_weight': 0.7378847160266617, 'loss_type': 'weighted'}. Best is trial 3 with value: 0.49815940856933594.\n","/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:22:08,112] Trial 4 finished with value: 0.37413281202316284 and parameters: {'hidden_size': 96, 'num_layers': 1, 'dropout': 0.29158266867970384, 'learning_rate': 0.00022237816096958924, 'batch_size': 8, 'bidirectional': False, 'weight_decay': 3.193045993123819e-06, 'r2_weight': 0.6736014708665463, 'loss_type': 'weighted'}. Best is trial 3 with value: 0.49815940856933594.\n","/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:22:19,591] Trial 5 finished with value: 0.3929845690727234 and parameters: {'hidden_size': 192, 'num_layers': 1, 'dropout': 0.08611744787946551, 'learning_rate': 0.0008942113530982499, 'batch_size': 8, 'bidirectional': True, 'weight_decay': 1.0533325171423621e-06, 'r2_weight': 0.7532138061343537, 'loss_type': 'weighted'}. Best is trial 3 with value: 0.49815940856933594.\n","/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:22:57,852] Trial 6 finished with value: 0.16392487287521362 and parameters: {'hidden_size': 64, 'num_layers': 2, 'dropout': 0.4582333975422445, 'learning_rate': 1.5116754480138297e-05, 'batch_size': 8, 'bidirectional': True, 'weight_decay': 1.3549087528954486e-05, 'r2_weight': 0.9171783154063955, 'loss_type': 'weighted'}. Best is trial 3 with value: 0.49815940856933594.\n","/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:23:01,640] Trial 7 finished with value: -0.039704084396362305 and parameters: {'hidden_size': 96, 'num_layers': 4, 'dropout': 0.015254728200989665, 'learning_rate': 1.8913454408068285e-05, 'batch_size': 16, 'bidirectional': False, 'weight_decay': 1.439579783645679e-05, 'r2_weight': 0.941659587891273, 'loss_type': 'r2'}. Best is trial 3 with value: 0.49815940856933594.\n","/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:23:06,707] Trial 8 finished with value: 0.5971581935882568 and parameters: {'hidden_size': 256, 'num_layers': 3, 'dropout': 0.4557218367544936, 'learning_rate': 0.0009692955475887944, 'batch_size': 64, 'bidirectional': True, 'weight_decay': 1.7322699092134836e-05, 'r2_weight': 0.7095857501404146, 'loss_type': 'weighted'}. Best is trial 8 with value: 0.5971581935882568.\n","/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:23:30,346] Trial 9 finished with value: 0.39717966318130493 and parameters: {'hidden_size': 224, 'num_layers': 3, 'dropout': 0.3852634934625224, 'learning_rate': 0.0037868660145843006, 'batch_size': 8, 'bidirectional': False, 'weight_decay': 0.0008352379680474077, 'r2_weight': 0.6645221747624325, 'loss_type': 'r2'}. Best is trial 8 with value: 0.5971581935882568.\n","/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:23:37,545] Trial 10 finished with value: 0.4909876585006714 and parameters: {'hidden_size': 256, 'num_layers': 2, 'dropout': 0.15836318945514, 'learning_rate': 0.009012897543533271, 'batch_size': 64, 'bidirectional': True, 'weight_decay': 8.17164153149605e-05, 'r2_weight': 0.51679454724914, 'loss_type': 'weighted'}. Best is trial 8 with value: 0.5971581935882568.\n","/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:23:42,816] Trial 11 finished with value: 0.4704590439796448 and parameters: {'hidden_size': 256, 'num_layers': 3, 'dropout': 0.38879876638963395, 'learning_rate': 0.0007065260202999694, 'batch_size': 64, 'bidirectional': True, 'weight_decay': 6.720194060524537e-05, 'r2_weight': 0.8063985916264813, 'loss_type': 'weighted'}. Best is trial 8 with value: 0.5971581935882568.\n","/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:23:53,258] Trial 12 finished with value: 0.3369848132133484 and parameters: {'hidden_size': 192, 'num_layers': 4, 'dropout': 0.23683590089489184, 'learning_rate': 0.00012078458380834623, 'batch_size': 64, 'bidirectional': True, 'weight_decay': 4.0458617286835245e-05, 'r2_weight': 0.5643523033345242, 'loss_type': 'weighted'}. Best is trial 8 with value: 0.5971581935882568.\n","/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:24:06,266] Trial 13 finished with value: 0.310710608959198 and parameters: {'hidden_size': 224, 'num_layers': 3, 'dropout': 0.4976340963324279, 'learning_rate': 6.817065520299369e-05, 'batch_size': 32, 'bidirectional': False, 'weight_decay': 2.7264229626202855e-06, 'r2_weight': 0.7853251441215521, 'loss_type': 'weighted'}. Best is trial 8 with value: 0.5971581935882568.\n","/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:24:09,081] Trial 14 finished with value: 0.5545662641525269 and parameters: {'hidden_size': 256, 'num_layers': 2, 'dropout': 0.34797642370920456, 'learning_rate': 0.002051493196320151, 'batch_size': 64, 'bidirectional': False, 'weight_decay': 7.518709628352596e-06, 'r2_weight': 0.8680162263061582, 'loss_type': 'weighted'}. Best is trial 8 with value: 0.5971581935882568.\n","/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:24:12,457] Trial 15 finished with value: 0.0 and parameters: {'hidden_size': 192, 'num_layers': 2, 'dropout': 0.2197962137228035, 'learning_rate': 0.004192719756572951, 'batch_size': 64, 'bidirectional': True, 'weight_decay': 2.433393745042033e-05, 'r2_weight': 0.865828808855082, 'loss_type': 'weighted'}. Best is trial 8 with value: 0.5971581935882568.\n"]},{"output_type":"stream","name":"stdout","text":["Trial failed with error: \n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:24:18,616] Trial 16 finished with value: 0.4979878067970276 and parameters: {'hidden_size': 224, 'num_layers': 2, 'dropout': 0.4310468370116307, 'learning_rate': 0.0005117979565484107, 'batch_size': 64, 'bidirectional': False, 'weight_decay': 0.00019797683113151066, 'r2_weight': 0.8617371028495968, 'loss_type': 'weighted'}. Best is trial 8 with value: 0.5971581935882568.\n","/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:24:23,531] Trial 17 finished with value: 0.48867732286453247 and parameters: {'hidden_size': 160, 'num_layers': 1, 'dropout': 0.3350041972041576, 'learning_rate': 0.006607323782761405, 'batch_size': 64, 'bidirectional': True, 'weight_decay': 1.0093851936954522e-06, 'r2_weight': 0.8685984249658317, 'loss_type': 'weighted'}. Best is trial 8 with value: 0.5971581935882568.\n","/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:24:27,101] Trial 18 finished with value: 0.5083264708518982 and parameters: {'hidden_size': 256, 'num_layers': 2, 'dropout': 0.2987533134622039, 'learning_rate': 0.0021730186875510977, 'batch_size': 64, 'bidirectional': False, 'weight_decay': 7.187488001676156e-06, 'r2_weight': 0.6080004049002301, 'loss_type': 'r2'}. Best is trial 8 with value: 0.5971581935882568.\n","/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:24:32,262] Trial 19 finished with value: 0.49919408559799194 and parameters: {'hidden_size': 224, 'num_layers': 3, 'dropout': 0.422258439698032, 'learning_rate': 0.0004763800110937042, 'batch_size': 64, 'bidirectional': True, 'weight_decay': 2.5954148472922226e-05, 'r2_weight': 0.7232895229118335, 'loss_type': 'weighted'}. Best is trial 8 with value: 0.5971581935882568.\n","/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:24:52,045] Trial 20 finished with value: 0.3042648434638977 and parameters: {'hidden_size': 160, 'num_layers': 2, 'dropout': 0.1997455744019811, 'learning_rate': 4.468554676503934e-05, 'batch_size': 16, 'bidirectional': True, 'weight_decay': 0.00012288807847566296, 'r2_weight': 0.6999482416726956, 'loss_type': 'weighted'}. Best is trial 8 with value: 0.5971581935882568.\n","/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:24:55,521] Trial 21 finished with value: 0.0 and parameters: {'hidden_size': 256, 'num_layers': 2, 'dropout': 0.3115768673720824, 'learning_rate': 0.0020023298767607067, 'batch_size': 64, 'bidirectional': False, 'weight_decay': 7.791613365253689e-06, 'r2_weight': 0.5990826514616758, 'loss_type': 'r2'}. Best is trial 8 with value: 0.5971581935882568.\n"]},{"output_type":"stream","name":"stdout","text":["Trial failed with error: \n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:24:58,847] Trial 22 finished with value: 0.2775992751121521 and parameters: {'hidden_size': 256, 'num_layers': 2, 'dropout': 0.27095024735137674, 'learning_rate': 0.0014782030935480712, 'batch_size': 64, 'bidirectional': False, 'weight_decay': 7.052884897232424e-06, 'r2_weight': 0.5625026758218589, 'loss_type': 'r2'}. Best is trial 8 with value: 0.5971581935882568.\n","/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:25:01,682] Trial 23 finished with value: 0.0 and parameters: {'hidden_size': 224, 'num_layers': 3, 'dropout': 0.3493185291062939, 'learning_rate': 0.003814724461795439, 'batch_size': 64, 'bidirectional': False, 'weight_decay': 1.5328994745833783e-05, 'r2_weight': 0.6235408161952354, 'loss_type': 'r2'}. Best is trial 8 with value: 0.5971581935882568.\n"]},{"output_type":"stream","name":"stdout","text":["Trial failed with error: \n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:25:07,603] Trial 24 finished with value: 0.0 and parameters: {'hidden_size': 256, 'num_layers': 1, 'dropout': 0.4153868353810287, 'learning_rate': 0.0011161014795740898, 'batch_size': 32, 'bidirectional': False, 'weight_decay': 1.6651948152210732e-06, 'r2_weight': 0.5195566871985049, 'loss_type': 'r2'}. Best is trial 8 with value: 0.5971581935882568.\n"]},{"output_type":"stream","name":"stdout","text":["Trial failed with error: \n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:25:09,754] Trial 25 finished with value: 0.41530555486679077 and parameters: {'hidden_size': 192, 'num_layers': 2, 'dropout': 0.1728484385595839, 'learning_rate': 0.0026533674881479077, 'batch_size': 64, 'bidirectional': False, 'weight_decay': 3.872157329168815e-05, 'r2_weight': 0.7738920094912641, 'loss_type': 'r2'}. Best is trial 8 with value: 0.5971581935882568.\n","/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:25:16,570] Trial 26 finished with value: 0.36135774850845337 and parameters: {'hidden_size': 224, 'num_layers': 3, 'dropout': 0.4945501346813915, 'learning_rate': 0.0003736213335974827, 'batch_size': 64, 'bidirectional': False, 'weight_decay': 5.851159813767468e-06, 'r2_weight': 0.82585504009522, 'loss_type': 'r2'}. Best is trial 8 with value: 0.5971581935882568.\n","/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:25:18,472] Trial 27 finished with value: 0.2718954086303711 and parameters: {'hidden_size': 256, 'num_layers': 2, 'dropout': 0.255484366738836, 'learning_rate': 0.00550804604324335, 'batch_size': 64, 'bidirectional': False, 'weight_decay': 1.0818393744450456e-05, 'r2_weight': 0.7042071776211354, 'loss_type': 'weighted'}. Best is trial 8 with value: 0.5971581935882568.\n","/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:25:21,089] Trial 28 finished with value: 0.4339693784713745 and parameters: {'hidden_size': 224, 'num_layers': 1, 'dropout': 0.31644425925419517, 'learning_rate': 0.00284335551470781, 'batch_size': 64, 'bidirectional': True, 'weight_decay': 2.2475583950356714e-06, 'r2_weight': 0.6022287919388984, 'loss_type': 'r2'}. Best is trial 8 with value: 0.5971581935882568.\n","/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:25:31,767] Trial 29 finished with value: 0.47816115617752075 and parameters: {'hidden_size': 192, 'num_layers': 3, 'dropout': 0.4450644420847844, 'learning_rate': 0.0007538719143505288, 'batch_size': 32, 'bidirectional': True, 'weight_decay': 2.014360468921274e-05, 'r2_weight': 0.8994821586605771, 'loss_type': 'r2'}. Best is trial 8 with value: 0.5971581935882568.\n","/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:25:51,917] Trial 30 finished with value: 0.6144900321960449 and parameters: {'hidden_size': 256, 'num_layers': 3, 'dropout': 0.12443573657858692, 'learning_rate': 0.009988439410223473, 'batch_size': 16, 'bidirectional': False, 'weight_decay': 4.691096730777323e-06, 'r2_weight': 0.5541537325865673, 'loss_type': 'weighted'}. Best is trial 30 with value: 0.6144900321960449.\n","/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:26:14,019] Trial 31 finished with value: 0.5908578038215637 and parameters: {'hidden_size': 256, 'num_layers': 3, 'dropout': 0.11282826500951246, 'learning_rate': 0.007043230606143745, 'batch_size': 16, 'bidirectional': False, 'weight_decay': 4.428361283391723e-06, 'r2_weight': 0.5622093828848387, 'loss_type': 'weighted'}. Best is trial 30 with value: 0.6144900321960449.\n","/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:26:32,328] Trial 32 finished with value: 0.42846810817718506 and parameters: {'hidden_size': 256, 'num_layers': 3, 'dropout': 0.11520789059770555, 'learning_rate': 0.00937767230063874, 'batch_size': 16, 'bidirectional': False, 'weight_decay': 4.665392888637853e-06, 'r2_weight': 0.5622807503684891, 'loss_type': 'weighted'}. Best is trial 30 with value: 0.6144900321960449.\n","/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:26:53,550] Trial 33 finished with value: 0.45082467794418335 and parameters: {'hidden_size': 224, 'num_layers': 3, 'dropout': 0.027332751564013755, 'learning_rate': 0.007191914540012348, 'batch_size': 16, 'bidirectional': False, 'weight_decay': 2.0030221618444774e-06, 'r2_weight': 0.5011023355922112, 'loss_type': 'weighted'}. Best is trial 30 with value: 0.6144900321960449.\n","/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:27:11,186] Trial 34 finished with value: 0.41432929039001465 and parameters: {'hidden_size': 128, 'num_layers': 4, 'dropout': 0.07839521522463565, 'learning_rate': 0.0012546683468403306, 'batch_size': 16, 'bidirectional': False, 'weight_decay': 4.109716247589454e-06, 'r2_weight': 0.644499267604704, 'loss_type': 'weighted'}. Best is trial 30 with value: 0.6144900321960449.\n","/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:27:26,330] Trial 35 finished with value: 0.0 and parameters: {'hidden_size': 256, 'num_layers': 3, 'dropout': 0.1352934745043792, 'learning_rate': 0.0050643758392972275, 'batch_size': 16, 'bidirectional': False, 'weight_decay': 1.1018334004454257e-05, 'r2_weight': 0.5489589904600697, 'loss_type': 'weighted'}. Best is trial 30 with value: 0.6144900321960449.\n"]},{"output_type":"stream","name":"stdout","text":["Trial failed with error: \n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:27:41,735] Trial 36 finished with value: 0.0 and parameters: {'hidden_size': 224, 'num_layers': 4, 'dropout': 0.06274206888090153, 'learning_rate': 0.009583400947845698, 'batch_size': 16, 'bidirectional': False, 'weight_decay': 3.5700427801900194e-06, 'r2_weight': 0.5818088051752742, 'loss_type': 'weighted'}. Best is trial 30 with value: 0.6144900321960449.\n"]},{"output_type":"stream","name":"stdout","text":["Trial failed with error: \n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:27:46,425] Trial 37 finished with value: 0.0 and parameters: {'hidden_size': 256, 'num_layers': 3, 'dropout': 0.1137029206226246, 'learning_rate': 0.0030904533215688904, 'batch_size': 16, 'bidirectional': False, 'weight_decay': 1.5087630804095705e-06, 'r2_weight': 0.6449100538414796, 'loss_type': 'weighted'}. Best is trial 30 with value: 0.6144900321960449.\n"]},{"output_type":"stream","name":"stdout","text":["Trial failed with error: \n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:27:56,506] Trial 38 finished with value: 0.3127838373184204 and parameters: {'hidden_size': 64, 'num_layers': 3, 'dropout': 0.04296772241782301, 'learning_rate': 0.00026347769316222523, 'batch_size': 16, 'bidirectional': False, 'weight_decay': 4.74071695877656e-06, 'r2_weight': 0.7523481035914932, 'loss_type': 'weighted'}. Best is trial 30 with value: 0.6144900321960449.\n","/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:28:09,092] Trial 39 finished with value: 0.0 and parameters: {'hidden_size': 256, 'num_layers': 3, 'dropout': 0.19088794838045514, 'learning_rate': 0.0017726125088719732, 'batch_size': 16, 'bidirectional': True, 'weight_decay': 8.830850447594251e-06, 'r2_weight': 0.6856412181223192, 'loss_type': 'weighted'}. Best is trial 30 with value: 0.6144900321960449.\n"]},{"output_type":"stream","name":"stdout","text":["Trial failed with error: \n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:28:27,977] Trial 40 finished with value: 0.0 and parameters: {'hidden_size': 224, 'num_layers': 4, 'dropout': 0.13795224982515703, 'learning_rate': 0.005962911924523671, 'batch_size': 8, 'bidirectional': False, 'weight_decay': 1.854875855690239e-05, 'r2_weight': 0.5399655030086334, 'loss_type': 'weighted'}. Best is trial 30 with value: 0.6144900321960449.\n"]},{"output_type":"stream","name":"stdout","text":["Trial failed with error: \n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:28:32,627] Trial 41 finished with value: 0.4930468797683716 and parameters: {'hidden_size': 256, 'num_layers': 2, 'dropout': 0.2648939643742546, 'learning_rate': 0.0021755913710954024, 'batch_size': 32, 'bidirectional': False, 'weight_decay': 7.123542110405109e-06, 'r2_weight': 0.6087705553450549, 'loss_type': 'weighted'}. Best is trial 30 with value: 0.6144900321960449.\n","/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:28:42,674] Trial 42 finished with value: 0.4936804175376892 and parameters: {'hidden_size': 256, 'num_layers': 2, 'dropout': 0.3624166963868557, 'learning_rate': 0.0008930109478388087, 'batch_size': 16, 'bidirectional': False, 'weight_decay': 3.010406792240199e-06, 'r2_weight': 0.5309140641688749, 'loss_type': 'weighted'}. Best is trial 30 with value: 0.6144900321960449.\n","/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:28:48,060] Trial 43 finished with value: 0.3810805082321167 and parameters: {'hidden_size': 256, 'num_layers': 2, 'dropout': 0.00020002224191456586, 'learning_rate': 0.003959190575535266, 'batch_size': 64, 'bidirectional': False, 'weight_decay': 5.415487422201185e-06, 'r2_weight': 0.5913064605991962, 'loss_type': 'weighted'}. Best is trial 30 with value: 0.6144900321960449.\n","/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:29:04,051] Trial 44 finished with value: 0.40969306230545044 and parameters: {'hidden_size': 256, 'num_layers': 3, 'dropout': 0.28999669350739066, 'learning_rate': 0.0013949032770106443, 'batch_size': 8, 'bidirectional': False, 'weight_decay': 1.1110003338482772e-05, 'r2_weight': 0.6322551756759687, 'loss_type': 'r2'}. Best is trial 30 with value: 0.6144900321960449.\n","/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:29:09,548] Trial 45 finished with value: 0.5655471086502075 and parameters: {'hidden_size': 224, 'num_layers': 3, 'dropout': 0.3944271236830561, 'learning_rate': 0.002467778930561199, 'batch_size': 64, 'bidirectional': False, 'weight_decay': 0.0007979933862985571, 'r2_weight': 0.67009397790407, 'loss_type': 'weighted'}. Best is trial 30 with value: 0.6144900321960449.\n","/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:29:18,195] Trial 46 finished with value: 0.0 and parameters: {'hidden_size': 224, 'num_layers': 3, 'dropout': 0.4748533510899219, 'learning_rate': 0.007349484137182986, 'batch_size': 16, 'bidirectional': True, 'weight_decay': 0.0005955528297331337, 'r2_weight': 0.9496606884113695, 'loss_type': 'weighted'}. Best is trial 30 with value: 0.6144900321960449.\n"]},{"output_type":"stream","name":"stdout","text":["Trial failed with error: \n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:29:20,314] Trial 47 finished with value: -0.22082924842834473 and parameters: {'hidden_size': 224, 'num_layers': 3, 'dropout': 0.39535878809012265, 'learning_rate': 0.004469045201536594, 'batch_size': 64, 'bidirectional': False, 'weight_decay': 5.178848250060143e-05, 'r2_weight': 0.7313582394446426, 'loss_type': 'weighted'}. Best is trial 30 with value: 0.6144900321960449.\n","/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:29:50,262] Trial 48 finished with value: 0.0 and parameters: {'hidden_size': 192, 'num_layers': 3, 'dropout': 0.4643457009078163, 'learning_rate': 0.00014724518496324958, 'batch_size': 8, 'bidirectional': True, 'weight_decay': 0.00025632407105642577, 'r2_weight': 0.6723189337685385, 'loss_type': 'weighted'}. Best is trial 30 with value: 0.6144900321960449.\n"]},{"output_type":"stream","name":"stdout","text":["Trial failed with error: \n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","[I 2025-05-12 21:29:54,831] Trial 49 finished with value: 0.460232138633728 and parameters: {'hidden_size': 96, 'num_layers': 4, 'dropout': 0.39966546929109525, 'learning_rate': 0.0031704892420212996, 'batch_size': 64, 'bidirectional': False, 'weight_decay': 0.0006404365857148474, 'r2_weight': 0.8169089832866363, 'loss_type': 'weighted'}. Best is trial 30 with value: 0.6144900321960449.\n"]},{"output_type":"stream","name":"stdout","text":["Best trial:\n","R² Score: 0.6144900321960449\n","Best hyperparameters:\n","  hidden_size: 256\n","  num_layers: 3\n","  dropout: 0.12443573657858692\n","  learning_rate: 0.009988439410223473\n","  batch_size: 16\n","  bidirectional: False\n","  weight_decay: 4.691096730777323e-06\n","  r2_weight: 0.5541537325865673\n","  loss_type: weighted\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","  7%|▋         | 10/150 [00:11<02:44,  1.17s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 10/150, Train Loss: 27116.9433317, Val MSE: 0.00000016, Val R²: 0.4572470, Val Corr: 0.6916823\n"]},{"output_type":"stream","name":"stderr","text":[" 13%|█▎        | 20/150 [00:23<02:33,  1.18s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 20/150, Train Loss: 32522.4166790, Val MSE: 0.00000017, Val R²: 0.4178913, Val Corr: 0.6652206\n"]},{"output_type":"stream","name":"stderr","text":[" 20%|██        | 30/150 [00:35<02:23,  1.19s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 30/150, Train Loss: 18756.0796405, Val MSE: 0.00000010, Val R²: 0.6564042, Val Corr: 0.8136330\n"]},{"output_type":"stream","name":"stderr","text":[" 27%|██▋       | 40/150 [00:47<02:09,  1.18s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 40/150, Train Loss: 24392.2350660, Val MSE: 0.00000011, Val R²: 0.6085753, Val Corr: 0.8138711\n"]},{"output_type":"stream","name":"stderr","text":[" 33%|███▎      | 50/150 [00:58<01:58,  1.18s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 50/150, Train Loss: 21632.0731224, Val MSE: 0.00000010, Val R²: 0.6349158, Val Corr: 0.8250990\n"]},{"output_type":"stream","name":"stderr","text":[" 40%|████      | 60/150 [01:10<01:45,  1.18s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 60/150, Train Loss: 16656.2683198, Val MSE: 0.00000011, Val R²: 0.6123277, Val Corr: 0.8258753\n"]},{"output_type":"stream","name":"stderr","text":[" 47%|████▋     | 70/150 [01:22<01:34,  1.18s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 70/150, Train Loss: 15482.3735398, Val MSE: 0.00000010, Val R²: 0.6351924, Val Corr: 0.8225663\n"]},{"output_type":"stream","name":"stderr","text":[" 49%|████▊     | 73/150 [01:26<01:31,  1.19s/it]"]},{"output_type":"stream","name":"stdout","text":["Early stopping at epoch 74\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Final Test MSE: 0.00000011\n","Final Test R²: 0.6005586\n","Final Test Correlation: 0.7821\n","\n","Starting two-stage training approach...\n","Stage 1: MSE Optimization\n","Stage 1 - Epoch 10/50, Train Loss: 0.0893499, Val MSE: 0.08112255, Val R²: 0.3287996\n","Stage 1 - Epoch 20/50, Train Loss: 0.0806374, Val MSE: 0.08532406, Val R²: 0.2940367\n","Stage 1 - Epoch 30/50, Train Loss: 0.0522640, Val MSE: 0.07234387, Val R²: 0.4014336\n","Stage 1 - Epoch 40/50, Train Loss: 0.0517519, Val MSE: 0.05842129, Val R²: 0.5166277\n","Stage 1 - Epoch 50/50, Train Loss: 0.0533778, Val MSE: 0.07524016, Val R²: 0.3774699\n","\n","Stage 2: R² Optimization\n","Stage 2 - Epoch 5/50, Train Loss: -0.3732480, Val R²: 0.4180571, Val MSE: 0.00000017\n","Stage 2 - Epoch 10/50, Train Loss: -0.2204035, Val R²: 0.4449029, Val MSE: 0.00000016\n","Stage 2 - Epoch 15/50, Train Loss: -0.3720224, Val R²: 0.4472895, Val MSE: 0.00000016\n","Early stopping stage 2 at epoch 17\n","\n","Two-Stage Model Results:\n","MSE: 0.00000018\n","R²: 0.3753374\n","\n","Training trend-focused model...\n","\n","Training trend-focused model...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning:\n","\n","The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","\n"]},{"output_type":"stream","name":"stdout","text":["Trend Model - Epoch 10/100, Train Loss: -0.5856354, Val R²: 0.3570626, Val MSE: 0.00000018\n","Trend Model - Epoch 20/100, Train Loss: -0.5482164, Val R²: 0.4144678, Val MSE: 0.00000017\n","Trend Model - Epoch 30/100, Train Loss: -0.7684197, Val R²: 0.4308795, Val MSE: 0.00000016\n","Trend Model - Epoch 40/100, Train Loss: -0.5608040, Val R²: 0.4680851, Val MSE: 0.00000015\n","Trend Model - Epoch 50/100, Train Loss: -0.7833325, Val R²: 0.4138120, Val MSE: 0.00000017\n","Early stopping trend model at epoch 60\n","\n","Trend Model Results:\n","MSE: 0.00000017\n","R²: 0.4119355\n","\n","Creating ultra ensemble solution...\n","Optimizing ultra ensemble weights...\n","New best weights: [0.0, 0.0, 1.0] - R²: 0.4119355, MSE: 0.00000017\n","New best weights: [0.0, 0.2, 0.8] - R²: 0.5223169, MSE: 0.00000014\n","New best weights: [0.0, 0.4, 0.6] - R²: 0.5738478, MSE: 0.00000012\n","New best weights: [0.2, 0.2, 0.6] - R²: 0.6075628, MSE: 0.00000011\n","New best weights: [0.2, 0.4, 0.4] - R²: 0.6177873, MSE: 0.00000011\n","New best weights: [0.4, 0.0, 0.6] - R²: 0.6206261, MSE: 0.00000011\n","New best weights: [0.4, 0.2, 0.4] - R²: 0.6483948, MSE: 0.00000010\n","New best weights: [0.6, 0.0, 0.4] - R²: 0.6583507, MSE: 0.00000010\n","\n","Final ultra ensemble with weights: [0.6, 0.0, 0.4]\n","\n","Ultra Ensemble Final Results:\n","MSE: 0.00000010\n","R²: 0.6583507\n","Correlation: 0.8245\n","\n","=== FINAL MODEL COMPARISON ===\n","Ensemble model:     R² = 0.6005586, MSE = 0.00000011\n","Two-stage model:    R² = 0.3753374, MSE = 0.00000018\n","Trend model:        R² = 0.4119355, MSE = 0.00000017\n","Ultra ensemble:     R² = 0.6583507, MSE = 0.00000010\n","============================\n"]}]},{"cell_type":"markdown","source":["REPRODUCE"],"metadata":{"id":"cJG4LMswylSA"}},{"cell_type":"code","source":["import os\n","import torch\n","import numpy as np\n","import random\n","import pandas as pd\n","from sklearn.metrics import mean_squared_error, r2_score\n","from torch import nn, optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.preprocessing import RobustScaler\n","from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n","from sklearn.model_selection import train_test_split\n","from tqdm import tqdm\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","model_type = \"bladder\"\n","\n","os.makedirs('/content/models', exist_ok=True)\n","\n","def set_all_seeds(seed=42):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","\n","class R2Loss(nn.Module):\n","    def __init__(self, eps=1e-6):\n","        super(R2Loss, self).__init__()\n","        self.eps = eps\n","\n","    def forward(self, y_pred, y_true):\n","        if torch.isnan(y_pred).any() or torch.isnan(y_true).any():\n","            y_pred = torch.nan_to_num(y_pred, nan=0.0)\n","            y_true = torch.nan_to_num(y_true, nan=0.0)\n","\n","        ss_tot = torch.sum((y_true - torch.mean(y_true)) ** 2) + self.eps\n","        ss_res = torch.sum((y_true - y_pred) ** 2) + self.eps\n","        r2 = 1 - ss_res / ss_tot\n","        return -r2\n","\n","class WeightedR2MSELoss(nn.Module):\n","    def __init__(self, r2_weight=0.8, mse_weight=0.2, eps=1e-6):\n","        super(WeightedR2MSELoss, self).__init__()\n","        self.r2_weight = r2_weight\n","        self.mse_weight = mse_weight\n","        self.r2_loss = R2Loss(eps=eps)\n","        self.mse_loss = nn.MSELoss()\n","\n","    def forward(self, y_pred, y_true):\n","        if torch.isnan(y_pred).any() or torch.isnan(y_true).any():\n","            y_pred = torch.nan_to_num(y_pred, nan=0.0)\n","            y_true = torch.nan_to_num(y_true, nan=0.0)\n","\n","        r2 = self.r2_loss(y_pred, y_true)\n","        mse = self.mse_loss(y_pred, y_true)\n","        return self.r2_weight * r2 + self.mse_weight * mse\n","\n","class AttentionLSTM(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, dropout, output_size=1, bidirectional=False):\n","        super(AttentionLSTM, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.bidirectional = bidirectional\n","        self.lstm = nn.LSTM(\n","            input_size=input_size,\n","            hidden_size=hidden_size,\n","            num_layers=num_layers,\n","            batch_first=True,\n","            dropout=dropout if num_layers > 1 else 0,\n","            bidirectional=bidirectional\n","        )\n","        self.dropout = nn.Dropout(dropout)\n","        self.attention = nn.Linear(hidden_size * (2 if bidirectional else 1), 1)\n","        self.fc1 = nn.Linear(hidden_size * (2 if bidirectional else 1), hidden_size)\n","        self.fc2 = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x):\n","        if torch.isnan(x).any():\n","            x = torch.nan_to_num(x, nan=0.0)\n","        batch_size, seq_len, features = x.size()\n","\n","        outputs, _ = self.lstm(x)\n","\n","        attention_weights = F.softmax(self.attention(outputs), dim=1)\n","        context_vector = torch.sum(attention_weights * outputs, dim=1)\n","\n","        x = F.gelu(self.fc1(context_vector))\n","        x = self.dropout(x)\n","        x = self.fc2(x)\n","\n","        if torch.isnan(x).any():\n","            x = torch.nan_to_num(x, nan=0.0)\n","\n","        return x\n","\n","class EnsembleModel(nn.Module):\n","    def __init__(self, models, weights=None):\n","        super(EnsembleModel, self).__init__()\n","        self.models = nn.ModuleList(models)\n","        if weights is None:\n","            self.weights = nn.Parameter(torch.ones(len(models)) / len(models))\n","        else:\n","            self.weights = nn.Parameter(torch.tensor(weights, dtype=torch.float32))\n","\n","    def forward(self, x):\n","        if torch.isnan(x).any():\n","            x = torch.nan_to_num(x, nan=0.0)\n","\n","        outputs = []\n","        for model in self.models:\n","            output = model(x)\n","            outputs.append(output)\n","\n","        stacked_outputs = torch.stack(outputs, dim=-1)\n","\n","        weights = F.softmax(self.weights, dim=0)\n","\n","        weighted_sum = torch.sum(stacked_outputs * weights, dim=-1)\n","\n","        if torch.isnan(weighted_sum).any():\n","            weighted_sum = torch.nan_to_num(weighted_sum, nan=0.0)\n","\n","        return weighted_sum\n","\n","class TrendModel(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size, dropout=0.2):\n","        super(TrendModel, self).__init__()\n","        self.fc1 = nn.Linear(input_size, hidden_size)\n","        self.bn1 = nn.BatchNorm1d(hidden_size)\n","\n","        self.fc2 = nn.Linear(hidden_size, hidden_size)\n","        self.bn2 = nn.BatchNorm1d(hidden_size)\n","\n","        self.fc3 = nn.Linear(hidden_size, hidden_size // 2)\n","        self.bn3 = nn.BatchNorm1d(hidden_size // 2)\n","\n","        self.fc4 = nn.Linear(hidden_size // 2, output_size)\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        batch_size = x.size(0)\n","        if torch.isnan(x).any():\n","            x = torch.nan_to_num(x, nan=0.0)\n","        if x.dim() > 2:\n","            x = x.reshape(batch_size, -1)\n","        x = self.fc1(x)\n","        if batch_size > 1:\n","            x = self.bn1(x)\n","        x = F.gelu(x)\n","        x = self.dropout(x)\n","        identity = x\n","        x = self.fc2(x)\n","        if batch_size > 1:\n","            x = self.bn2(x)\n","        x = F.gelu(x)\n","        x = self.dropout(x)\n","        x = x + identity\n","        x = self.fc3(x)\n","        if batch_size > 1:\n","            x = self.bn3(x)\n","        x = F.gelu(x)\n","        x = self.dropout(x)\n","        x = self.fc4(x)\n","        if torch.isnan(x).any():\n","            x = torch.nan_to_num(x, nan=0.0)\n","\n","        return x\n","\n","class TimeSeries(Dataset):\n","    def __init__(self, X: np.ndarray, y: np.ndarray, noise_level=0.0):\n","        self.original_X = X.copy()\n","        self.original_y = y.reshape(-1, 1) if y.ndim == 1 else y.copy()\n","\n","        self.x_scaler = RobustScaler(quantile_range=(5.0, 95.0))\n","        X_flat = np.nan_to_num(X.reshape(-1, X.shape[2]), nan=0.0)\n","        self.X_scaled = self.x_scaler.fit_transform(X_flat).reshape(X.shape)\n","        self.X_scaled = np.clip(self.X_scaled, -10.0, 10.0)\n","\n","        self.y_scaler = RobustScaler(quantile_range=(5.0, 95.0))\n","        self.y_scaled = self.y_scaler.fit_transform(self.original_y)\n","        self.y_scaled = np.clip(self.y_scaled, -10.0, 10.0)\n","\n","        self.noise_level = noise_level\n","\n","    def __len__(self):\n","        return len(self.X_scaled)\n","\n","    def __getitem__(self, idx):\n","        X_sample = self.X_scaled[idx].astype(np.float32)\n","        y_sample = self.y_scaled[idx].astype(np.float32)\n","\n","        if self.noise_level > 0:\n","            noise = np.random.normal(0, self.noise_level, X_sample.shape)\n","            X_sample = X_sample + noise\n","\n","        return torch.tensor(X_sample, dtype=torch.float32), torch.tensor(y_sample, dtype=torch.float32).view(-1)\n","\n","    def inverse_transform_y(self, y_tensor):\n","        if hasattr(y_tensor, \"detach\"):\n","            y_numpy = y_tensor.detach().cpu().numpy()\n","        else:\n","            y_numpy = y_tensor\n","        return self.y_scaler.inverse_transform(y_numpy)\n","\n","class TrendDataset(Dataset):\n","    def __init__(self, X: np.ndarray, y: np.ndarray, scale_y: bool = True):\n","        assert X.shape[0] == y.shape[0], \"Mismatched number of samples\"\n","        self.original_X = X\n","        self.original_y = y.reshape(-1, 1) if y.ndim == 1 else y\n","        self.x_scaler = RobustScaler(quantile_range=(5.0, 95.0))\n","        X_flat = np.nan_to_num(X.reshape(-1, X.shape[2]), nan=0.0)\n","        X_scaled = self.x_scaler.fit_transform(X_flat).reshape(X.shape)\n","        X_scaled = np.clip(X_scaled, -10.0, 10.0)\n","        self.n, self.t, self.f = X.shape\n","        self.features = []\n","        self.targets = []\n","        self.indices = []\n","        for i in range(self.n):\n","            x_sample = X_scaled[i]\n","            y_sample = y[i]\n","            if np.isnan(x_sample).any() or np.isnan(y_sample):\n","                continue\n","            if self.t > 1:\n","                x_pct_change = np.zeros((self.t-1, self.f))\n","                for j in range(self.t-1):\n","                    denom = np.abs(x_sample[j]) + 1e-8\n","                    x_pct_change[j] = (x_sample[j+1] - x_sample[j]) / denom\n","                x_pct_change = np.clip(x_pct_change, -10.0, 10.0)\n","                x_combined = np.column_stack([\n","                    x_pct_change.reshape(-1),\n","                    x_sample[:-1].reshape(-1)\n","                ])\n","\n","                self.features.append(x_combined)\n","                self.targets.append(y_sample)\n","                self.indices.append(i)\n","\n","        if len(self.features) > 0:\n","            self.features = np.array(self.features)\n","            self.targets = np.array(self.targets).reshape(-1, 1)\n","            self.indices = np.array(self.indices)\n","            self.scale_y = scale_y\n","            if scale_y:\n","                self.y_scaler = RobustScaler(quantile_range=(5.0, 95.0))\n","                self.targets_scaled = self.y_scaler.fit_transform(self.targets)\n","                self.targets_scaled = np.clip(self.targets_scaled, -10.0, 10.0)\n","            else:\n","                self.targets_scaled = self.targets\n","                self.y_scaler = None\n","        else:\n","            print(\"Warning: No valid samples found for trend dataset\")\n","            self.features = np.zeros((1, 1, self.f * 2))\n","            self.targets = np.zeros((1, 1))\n","            self.targets_scaled = np.zeros((1, 1))\n","            self.indices = np.zeros(1)\n","            self.scale_y = scale_y\n","            self.y_scaler = None\n","\n","    def __len__(self):\n","        return len(self.features)\n","\n","    def __getitem__(self, idx):\n","        return (\n","            torch.tensor(self.features[idx], dtype=torch.float32),\n","            torch.tensor(self.targets_scaled[idx], dtype=torch.float32).view(-1)\n","        )\n","\n","    def inverse_transform_y(self, y_tensor: torch.Tensor) -> np.ndarray:\n","        if self.scale_y and self.y_scaler is not None:\n","            return self.y_scaler.inverse_transform(y_tensor.detach().cpu().numpy())\n","        else:\n","            return y_tensor.detach().cpu().numpy()\n","\n","def train(model, criterion, optimizer, train_dataloader, scheduler=None):\n","    model.train()\n","    epoch_loss = 0.0\n","    for batch_X, batch_y in train_dataloader:\n","        if torch.isnan(batch_X).any() or torch.isnan(batch_y).any():\n","            batch_X = torch.nan_to_num(batch_X, nan=0.0)\n","            batch_y = torch.nan_to_num(batch_y, nan=0.0)\n","        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(batch_X)\n","        loss = criterion(outputs, batch_y)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","        optimizer.step()\n","        epoch_loss += loss.item()\n","\n","    if scheduler is not None:\n","        if isinstance(scheduler, ReduceLROnPlateau):\n","            scheduler.step(epoch_loss / len(train_dataloader))\n","        else:\n","            scheduler.step()\n","    return epoch_loss / len(train_dataloader)\n","\n","def validate(model, val_dataloader, dataset):\n","    model.eval()\n","    all_preds = []\n","    all_targets = []\n","\n","    with torch.no_grad():\n","        for batch_X, batch_y in val_dataloader:\n","            if torch.isnan(batch_X).any() or torch.isnan(batch_y).any():\n","                batch_X = torch.nan_to_num(batch_X, nan=0.0)\n","                batch_y = torch.nan_to_num(batch_y, nan=0.0)\n","            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n","            outputs = model(batch_X)\n","            if torch.isnan(outputs).any():\n","                outputs = torch.nan_to_num(outputs, nan=0.0)\n","            all_targets.append(batch_y.cpu())\n","            all_preds.append(outputs.cpu())\n","    all_preds = torch.cat(all_preds, dim=0)\n","    all_targets = torch.cat(all_targets, dim=0)\n","    all_preds = torch.nan_to_num(all_preds, nan=0.0)\n","    all_targets = torch.nan_to_num(all_targets, nan=0.0)\n","    np_preds = all_preds.numpy()\n","    np_targets = all_targets.numpy()\n","    mse = mean_squared_error(np_targets, np_preds)\n","    r2 = r2_score(np_targets, np_preds)\n","    try:\n","        corr = np.corrcoef(np_targets.flatten(), np_preds.flatten())[0, 1]\n","    except:\n","        corr = 0.0\n","    y_pred = dataset.inverse_transform_y(all_preds)\n","    y_true = dataset.inverse_transform_y(all_targets)\n","    true_mse = mean_squared_error(y_true, y_pred)\n","    true_r2 = r2_score(y_true, y_pred)\n","\n","    return mse, r2, corr, true_mse, true_r2\n","\n","def ultra_ensemble_predict(X_batch, ensemble_model, trend_model, weights=None):\n","    \"\"\"Combines predictions from ensemble models\"\"\"\n","    if weights is None:\n","        weights = [0.6, 0.0, 0.4]\n","\n","    X_batch = X_batch.to(device)\n","\n","    ensemble_model.eval()\n","    with torch.no_grad():\n","        ensemble_pred = ensemble_model(X_batch).cpu()\n","\n","    X_np = X_batch.cpu().numpy()\n","    n, t, f = X_np.shape\n","    trend_features = []\n","\n","    for i in range(n):\n","        x_sample = X_np[i]\n","        if t > 1:\n","            x_pct_change = np.zeros((t-1, f))\n","            for j in range(t-1):\n","                denom = np.abs(x_sample[j]) + 1e-8\n","                x_pct_change[j] = (x_sample[j+1] - x_sample[j]) / denom\n","            x_pct_change = np.clip(x_pct_change, -10.0, 10.0)\n","            x_combined = np.column_stack([\n","                x_pct_change.reshape(-1),\n","                x_sample[:-1].reshape(-1)\n","            ])\n","\n","            trend_features.append(x_combined)\n","\n","    if trend_features:\n","        trend_features = torch.tensor(np.array(trend_features), dtype=torch.float32).to(device)\n","        trend_model.eval()\n","        with torch.no_grad():\n","            trend_pred = trend_model(trend_features).cpu()\n","    else:\n","        trend_pred = ensemble_pred\n","\n","    final_pred = weights[0] * ensemble_pred + weights[2] * trend_pred\n","\n","    return final_pred\n","\n","def load_data():\n","    \"\"\"Loads your data files\"\"\"\n","    features = np.load(f'/content/{model_type}_features.npy')\n","    targets = np.load(f'/content/{model_type}_target.npy')\n","    index = np.load(f'/content/{model_type}_index.npy')\n","    return features, targets, index\n","\n"," r2, _, true_mse, true_r2 = validate(trend_model, trend_test_loader, trend_test_data)\n","            prdef reproduce_best_results():\n","    set_all_seeds(42)\n","\n","\n","    print(f\"Loading data for {model_type} model...\")\n","    features, targets, index = load_data()\n","\n","    best_params = {\n","        'hidden_size': 256,\n","        'num_layers': 3,\n","        'dropout': 0.12443573657858692,\n","        'learning_rate': 0.009988439410223473,\n","        'batch_size': 16,\n","        'bidirectional': False,\n","        'weight_decay': 4.691096730777323e-06,\n","        'r2_weight': 0.5541537325865673,\n","        'loss_type': 'weighted'\n","    }\n","\n","    hidden_size = best_params['hidden_size']\n","    num_layers = best_params['num_layers']\n","    dropout = best_params['dropout']\n","    learning_rate = best_params['learning_rate']\n","    batch_size = best_params['batch_size']\n","    bidirectional = best_params['bidirectional']\n","    weight_decay = best_params['weight_decay']\n","    r2_weight = best_params['r2_weight']\n","    mse_weight = 1.0 - r2_weight\n","    loss_type = best_params['loss_type']\n","\n","    X_train, X_test, y_train, y_test, _, index_test = train_test_split(\n","        features,\n","        targets,\n","        index,\n","        test_size=0.2,\n","        shuffle=True,\n","        stratify=index[:, 1],\n","        random_state=42\n","    )\n","\n","    train_data = TimeSeries(X_train, y_train, noise_level=0.0001)\n","    test_data = TimeSeries(X_test, y_test, noise_level=0)\n","    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=0)\n","    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=0)\n","\n","    print(\"\\nTraining ensemble model...\")\n","    models = []\n","    for i in range(3):\n","        set_all_seeds(42 + i)\n","        h_size = int(hidden_size * (0.9 + 0.2 * np.random.random()))\n","        d_out = dropout * (0.8 + 0.4 * np.random.random())\n","\n","        model = AttentionLSTM(\n","            input_size=features.shape[2],\n","            hidden_size=h_size,\n","            num_layers=num_layers,\n","            dropout=d_out,\n","            output_size=1,\n","            bidirectional=bidirectional\n","        ).to(device)\n","\n","        models.append(model)\n","\n","    set_all_seeds(42)\n","    ensemble_model = EnsembleModel(models, weights=[0.5, 0.3, 0.2]).to(device)\n","\n","    if loss_type == \"r2\":\n","        criterion = R2Loss()\n","    else:\n","        criterion = WeightedR2MSELoss(r2_weight=r2_weight, mse_weight=mse_weight)\n","\n","    optimizer = optim.AdamW(ensemble_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n","    scheduler1 = ReduceLROnPlateau(optimizer, mode='min', factor=0.7, patience=10, verbose=False, min_lr=1e-6)\n","    scheduler2 = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=learning_rate/20)\n","\n","    num_epochs = 150\n","    for epoch in tqdm(range(num_epochs)):\n","        train_loss = train(ensemble_model, criterion, optimizer, train_loader)\n","        scheduler2.step()\n","        mse, r2, corr, true_mse, true_r2 = validate(ensemble_model, test_loader, test_data)\n","        scheduler1.step(train_loss)\n","\n","        if (epoch + 1) % 10 == 0:\n","            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.7f}, \"\n","                  f\"Val MSE: {true_mse:.8f}, Val R²: {true_r2:.7f}, Val Corr: {corr:.7f}\")\n","\n","    _, _, _, ensemble_mse, ensemble_r2 = validate(ensemble_model, test_loader, test_data)\n","    print(f\"Ensemble Model Results: MSE: {ensemble_mse:.8f}, R²: {ensemble_r2:.7f}\")\n","    print(\"\\nTraining trend model...\")\n","    set_all_seeds(42)\n","\n","    trend_train_data = TrendDataset(X_train, y_train)\n","    trend_test_data = TrendDataset(X_test, y_test)\n","    input_dim = trend_train_data.features.shape[1] * trend_train_data.features.shape[2]\n","    trend_train_loader = DataLoader(trend_train_data, batch_size=32, shuffle=True, num_workers=0)\n","    trend_test_loader = DataLoader(trend_test_data, batch_size=32, shuffle=False, num_workers=0)\n","\n","    trend_model = TrendModel(\n","        input_size=input_dim,\n","        hidden_size=128,\n","        output_size=1,\n","        dropout=0.2\n","    ).to(device)\n","\n","    trend_criterion = R2Loss()\n","    trend_optimizer = optim.AdamW(trend_model.parameters(), lr=0.001, weight_decay=1e-5)\n","    trend_scheduler = ReduceLROnPlateau(trend_optimizer, mode='min', factor=0.5, patience=10, verbose=False)\n","\n","    trend_epochs = 60\n","    for epoch in tqdm(range(trend_epochs)):\n","        train_loss = train(trend_model, trend_criterion, trend_optimizer, trend_train_loader, trend_scheduler)\n","\n","        if (epoch + 1) % 10 == 0:\n","            _,int(f\"Trend Model - Epoch {epoch+1}/{trend_epochs}, Train Loss: {train_loss:.7f}, \"\n","                  f\"Val R²: {true_r2:.7f}, Val MSE: {true_mse:.8f}\")\n","\n","    _, _, _, trend_mse, trend_r2 = validate(trend_model, trend_test_loader, trend_test_data)\n","    print(f\"Trend Model Results: MSE: {trend_mse:.8f}, R²: {trend_r2:.7f}\")\n","\n","    print(\"\\nEvaluating ultra ensemble with fixed weights: [0.6, 0.0, 0.4]\")\n","    ultra_weights = [0.6, 0.0, 0.4]\n","\n","    all_preds = []\n","    all_targets = []\n","\n","    for X_batch, y_batch in test_loader:\n","        pred = ultra_ensemble_predict(X_batch, ensemble_model, trend_model, ultra_weights)\n","        all_preds.append(pred)\n","        all_targets.append(y_batch)\n","\n","    y_pred_tensor = torch.cat(all_preds)\n","    y_true_tensor = torch.cat(all_targets)\n","\n","    y_pred = test_data.inverse_transform_y(y_pred_tensor)\n","    y_true = test_data.inverse_transform_y(y_true_tensor)\n","\n","    ultra_mse = mean_squared_error(y_true, y_pred)\n","    ultra_r2 = r2_score(y_true, y_pred)\n","    ultra_corr = np.corrcoef(y_true.flatten(), y_pred.flatten())[0, 1]\n","\n","    print(\"\\nUltra Ensemble Final Results:\")\n","    print(f\"MSE: {ultra_mse:.8f}\")\n","    print(f\"R²: {ultra_r2:.7f}\")\n","    print(f\"Correlation: {ultra_corr:.4f}\")\n","\n","    save_path = f\"/content/models/{model_type}_reproducible_model.pth\"\n","    torch.save({\n","        'ensemble_model': ensemble_model.state_dict(),\n","        'trend_model': trend_model.state_dict(),\n","        'weights': ultra_weights,\n","        'params': best_params,\n","        'performance': {'r2': ultra_r2, 'mse': ultra_mse, 'corr': ultra_corr}\n","    }, save_path)\n","\n","    print(f\"Saved reproducible model to {save_path}\")\n","\n","    return {\n","        'ensemble_model': ensemble_model,\n","        'trend_model': trend_model,\n","        'weights': ultra_weights,\n","        'params': best_params,\n","        'performance': {'r2': ultra_r2, 'mse': ultra_mse, 'corr': ultra_corr}\n","    }\n","\n","if __name__ == \"__main__\":\n","    model_result = reproduce_best_results()\n","    print(\"\\nPROCESS COMPLETED\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yq4cMV4k6CjO","executionInfo":{"status":"ok","timestamp":1747095345898,"user_tz":240,"elapsed":149207,"user":{"displayName":"Danielle Grunwald","userId":"18017908855650010663"}},"outputId":"b906171f-44c5-42c4-c87f-669f7e20c503"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Loading data for bladder model...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning:\n","\n","The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","\n"]},{"output_type":"stream","name":"stdout","text":["\n","Training ensemble model...\n"]},{"output_type":"stream","name":"stderr","text":["  7%|▋         | 10/150 [00:09<02:08,  1.09it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 10/150, Train Loss: -0.2601079, Val MSE: 0.00000019, Val R²: 0.3383859, Val Corr: 0.5918068\n"]},{"output_type":"stream","name":"stderr","text":[" 13%|█▎        | 20/150 [00:18<02:06,  1.03it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 20/150, Train Loss: -0.0673989, Val MSE: 0.00000017, Val R²: 0.4017236, Val Corr: 0.6404962\n"]},{"output_type":"stream","name":"stderr","text":[" 20%|██        | 30/150 [00:28<01:58,  1.01it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 30/150, Train Loss: -0.2580888, Val MSE: 0.00000012, Val R²: 0.5906613, Val Corr: 0.7686763\n"]},{"output_type":"stream","name":"stderr","text":[" 27%|██▋       | 40/150 [00:38<01:42,  1.08it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 40/150, Train Loss: -0.0732837, Val MSE: 0.00000015, Val R²: 0.4845495, Val Corr: 0.7056020\n"]},{"output_type":"stream","name":"stderr","text":[" 33%|███▎      | 50/150 [00:47<01:32,  1.08it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 50/150, Train Loss: -0.2423286, Val MSE: 0.00000015, Val R²: 0.4808832, Val Corr: 0.6954632\n"]},{"output_type":"stream","name":"stderr","text":[" 40%|████      | 60/150 [00:56<01:23,  1.08it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 60/150, Train Loss: -0.2988521, Val MSE: 0.00000011, Val R²: 0.6008077, Val Corr: 0.7860671\n"]},{"output_type":"stream","name":"stderr","text":[" 47%|████▋     | 70/150 [01:05<01:13,  1.08it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 70/150, Train Loss: -0.3254830, Val MSE: 0.00000012, Val R²: 0.5783157, Val Corr: 0.7763903\n"]},{"output_type":"stream","name":"stderr","text":[" 53%|█████▎    | 80/150 [01:15<01:05,  1.08it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 80/150, Train Loss: -0.2253699, Val MSE: 0.00000014, Val R²: 0.5155482, Val Corr: 0.7499434\n"]},{"output_type":"stream","name":"stderr","text":[" 60%|██████    | 90/150 [01:24<00:55,  1.08it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 90/150, Train Loss: -0.1619613, Val MSE: 0.00000016, Val R²: 0.4272923, Val Corr: 0.6579330\n"]},{"output_type":"stream","name":"stderr","text":[" 67%|██████▋   | 100/150 [01:33<00:46,  1.07it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 100/150, Train Loss: -0.2630970, Val MSE: 0.00000011, Val R²: 0.6260577, Val Corr: 0.7970059\n"]},{"output_type":"stream","name":"stderr","text":[" 73%|███████▎  | 110/150 [01:43<00:37,  1.08it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 110/150, Train Loss: -0.3045605, Val MSE: 0.00000012, Val R²: 0.5890200, Val Corr: 0.7889821\n"]},{"output_type":"stream","name":"stderr","text":[" 80%|████████  | 120/150 [01:52<00:27,  1.08it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 120/150, Train Loss: -0.3358959, Val MSE: 0.00000013, Val R²: 0.5574176, Val Corr: 0.7768703\n"]},{"output_type":"stream","name":"stderr","text":[" 87%|████████▋ | 130/150 [02:01<00:18,  1.09it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 130/150, Train Loss: -0.3356331, Val MSE: 0.00000012, Val R²: 0.5728933, Val Corr: 0.7757284\n"]},{"output_type":"stream","name":"stderr","text":[" 93%|█████████▎| 140/150 [02:10<00:09,  1.09it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 140/150, Train Loss: -0.3237687, Val MSE: 0.00000013, Val R²: 0.5475404, Val Corr: 0.7706282\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 150/150 [02:19<00:00,  1.07it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 150/150, Train Loss: -0.3548865, Val MSE: 0.00000012, Val R²: 0.5628289, Val Corr: 0.7770925\n","Ensemble Model Results: MSE: 0.00000012, R²: 0.5628289\n","\n","Training trend model...\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning:\n","\n","The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","\n"," 18%|█▊        | 11/60 [00:01<00:06,  7.20it/s]"]},{"output_type":"stream","name":"stdout","text":["Trend Model - Epoch 10/60, Train Loss: -0.6122513, Val R²: 0.3371849, Val MSE: 0.00000019\n"]},{"output_type":"stream","name":"stderr","text":[" 35%|███▌      | 21/60 [00:02<00:05,  7.30it/s]"]},{"output_type":"stream","name":"stdout","text":["Trend Model - Epoch 20/60, Train Loss: -0.7639372, Val R²: 0.3230658, Val MSE: 0.00000019\n"]},{"output_type":"stream","name":"stderr","text":[" 52%|█████▏    | 31/60 [00:04<00:03,  7.31it/s]"]},{"output_type":"stream","name":"stdout","text":["Trend Model - Epoch 30/60, Train Loss: -0.7693813, Val R²: 0.3926140, Val MSE: 0.00000017\n"]},{"output_type":"stream","name":"stderr","text":[" 68%|██████▊   | 41/60 [00:05<00:02,  7.05it/s]"]},{"output_type":"stream","name":"stdout","text":["Trend Model - Epoch 40/60, Train Loss: -0.8715709, Val R²: 0.3491151, Val MSE: 0.00000019\n"]},{"output_type":"stream","name":"stderr","text":[" 85%|████████▌ | 51/60 [00:07<00:01,  6.80it/s]"]},{"output_type":"stream","name":"stdout","text":["Trend Model - Epoch 50/60, Train Loss: -0.7570299, Val R²: 0.4151248, Val MSE: 0.00000017\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 60/60 [00:08<00:00,  7.24it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Trend Model - Epoch 60/60, Train Loss: -0.8796713, Val R²: 0.3757032, Val MSE: 0.00000018\n","Trend Model Results: MSE: 0.00000018, R²: 0.3757032\n","\n","Evaluating ultra ensemble with fixed weights: [0.6, 0.0, 0.4]\n","\n","Ultra Ensemble Final Results:\n","MSE: 0.00000011\n","R²: 0.6277786\n","Correlation: 0.7925\n","Saved reproducible model to /content/models/bladder_reproducible_model.pth\n","\n","PROCESS COMPLETED\n"]}]}]}